}
# Apply clustering and summarization
clustering_test <- thermal_aggregated_tbl %>%
# filter(cluster_type == "Hard Coal") %>%# for testing purposes, will speed things up
group_by(node, cluster_type) %>%
nest() %>%
mutate(
clustered_data = map(data, ~ cluster_and_summarize(.x, k = 10, node, cluster_type))
# the number k can be changed and generalized !
) %>%
# unnest_wider(clustered_data) %>%
unnest(clustered_data) %>%
select(generator_name, node, cluster_type, nominal_capacity, nb_units, min_stable_power, co2_emission, variable_cost, start_cost)
# select(-data)  # Remove `data` after unnesting
# View the result
print(clustering_test)
# Un test du 10-clustering (on est g√©n√©reux !) sur le Hard Coal fait passer
# de 1636 lignes √† 971.
# Essayons sur chaque type de thermique maintenant !
########
# It works !!
# The smart thing to do would be to generalize this method in the script right now.
# The lazy thing to do is to create an r_object and just run a simulation right now.
# Hm..
# Nevermind it doesn't work anymore..
# deja aggregate without clustering √ßa serait pas mal nn ? mdrrr
# faudrait faire un AGGREGATE_THERMAL et AGGREGATE_AND_CLUSTER_THERMAL
# tout comme un AGGREGATE_BATTERIES tout simple √ßa reviendrait juste
# √† changer la r√®gle / la boucle for
##########
# I have a question concerning clustering algorithms.
# Is it possible to generate automatically the amount of clusters,
# depending on the dataset and what would "make sense" intuitively ?
# For example, if I have values 10, 11, 10, 100, 110,
# it's pretty clear that 2 clusters is a good answer.
# But that might change from dataset to dataset.
## R√©ponse : oui mais c'est encore plus long. Il faut run plusieurs fois des k-means
# pour le quantifier. Pas tr√®s int√©ressant comme exercice ici par rapport
# √† juste dire "limiter √† 10 max" par exemple.
#
# Yes, automatically determining the optimal number of clusters in a dataset is a common challenge in clustering tasks. Several methods can help determine a reasonable number of clusters, each with its strengths and limitations. Here are some of the most commonly used methods:
#
#   1. Elbow Method
# The Elbow Method involves running k-means clustering with a range of cluster numbers
# ùëò
# k and plotting the sum of squared distances (within-cluster sum of squares) against
# ùëò
# k. The idea is to find the "elbow" point where adding more clusters yields only a small decrease in the sum of squared distances. The optimal number of clusters is often at this point.
# Apply the clustering and summarization within each country and fuel type
# clustering_test <- thermal_generators_properties_tbl %>%
#   group_by(node, cluster_type) %>%
#   nest()
#
# print(clustering_test)
#
#
# clustering_test <- clustering_test %>%
#   # mutate(
#   #   clustered_data = map2(data, node, cluster_type,
#   #                         ~ cluster_and_summarize(.x, k = 5, node = .y, cluster_type = ..3))
#   # ) %>%
#   mutate(clustered_data = map(data, cluster_and_summarize, k = 2, node, cluster_type)) %>%
#   unnest(clustered_data) %>%
#   select(-data)
# # View the result
# print(clustering_test)
# df <- thermal_generators_properties_tbl
# # Apply the clustering and summarization within each country and fuel type
# clustering_test <- df %>%
#   group_by(node, cluster_type) %>%
#   nest()
#
# print(clustering_test)
#
# clustering_test <- clustering_test %>%
#   mutate(clustered_data = map(data, cluster_and_summarize, k = 2, node, cluster_type)) %>%  # 'data' is the name of the nested tibble column
#   unnest(clustered_data) %>%
#   select(-data)  # Remove the original nested column
# # View the result
# print(clustering_test %>% filter(node == "AS-IND-EA" & cluster_type == "Hard Coal"))
print(clustering_test %>% filter(node == "AS-IND-EA" & cluster_type == "Hard Coal"))
print(thermal_aggregated_tbl %>% filter(node == "AS-IND-EA" & cluster_type == "Hard Coal"))
print(clustering_test %>% filter(node == "AS-IND-EA" & cluster_type == "Hard Coal"))
print(clustering_test)
print(thermal_aggregated_tbl)
saveRDS(object = clustering_test, file = ".\\src\\objects\\thermal_10clustering_tbl.rds")
clustering_test <- readRDS(".\\src\\objects\\thermal_10clustering_tbl.rds")
print(clustering_test)
print(clustering_test, n = 200)
# Load necessary libraries
library(dplyr)
library(tidyr)
library(purrr)
source(".\\src\\utils.R")
thermal_aggregated_tbl <- readRDS(".\\src\\objects\\thermal_aggregated_tbl.rds")
print(thermal_aggregated_tbl)
# NOTA BENE : v√©rifier avec la formule qu'on a pour les marginaux,
# que la valeur moyenne du cout marginal est bien le co√ªt marginal de la moyenne des centrales
# (genre, c'est s√ªrement une somme et du coup sommer/moyenner les centrales est ok)
# mais il faut tout de m√™me le justifier...
# Ce serait bien aussi de v√©rifier que le clustering se fait bien genre
# de print/log/observer les capacit√©s nominales r√©sultantes et voir que c'est ok
# tests unitaires que les unit√©s de la somme est bien la somme des unit√©s, etc...
# NB : explorer peut-√™tre diff√©rence entre k-means et k-medoids, et justifier
# choix de m√©thodes (ou assumer : k-medoids est mieux mais paltime)
# Function to perform clustering and aggregation
# Edit : no need for aggregation, it's now a feature of importThermal
cluster_and_summarize <- function(df, k, node, cluster_type) {
print("df:")
print(df)
# Check if the number of rows is greater than k
if (nrow(df) > k) {
# Perform k-means clustering on the aggregated data's nominal_capacity
clusters <- kmeans(df$nominal_capacity, centers = k)
df$cluster <- as.factor(clusters$cluster)
} else {
# If there are fewer rows than k, each row becomes its own cluster
df$cluster <- as.factor(1:nrow(df))
}
print("df with clusters")
print(df)
# # Join the cluster information back to the original dataframe
# df_clustered <- df %>%
#   left_join(aggregated_df %>% select(nominal_capacity, min_stable_power, co2_emission, variable_cost, start_cost, total_nb_units, cluster),
#             by = c("nominal_capacity", "min_stable_power", "co2_emission", "variable_cost", "start_cost")) %>%
#   mutate(cluster = aggregated_df$cluster[match(df$nominal_capacity, aggregated_df$nominal_capacity)])
#
# print(df_clustered)
# print("clustered df:")
# df <- df_clustered
# Summarize the clusters
summary <- df %>%
group_by(cluster) %>%
summarise(
combined_names = paste0(
unique(getPrefix(generator_name))[1],  # Extract and keep the prefix only once
paste(
unique(sapply(generator_name, removePrefix)),  # Remove the prefix and combine unique names
collapse = "_"
)
),
# generator_name = paste(unique(substring(gsub("^[^_]+_[^_]+_", "", generator_name), 1, 5)), collapse = "_"),
# generator_name = {
#   prefix <- unique(getPrefix(generator_name))[1]
#   combined_names <- paste(unique(substring(removePrefix(generator_name), 1, 5)), collapse = "_")
#   truncateStringVec(paste0(prefix, combined_names), 88)
# },
nominal_capacity = mean(nominal_capacity),
nb_units = sum(nb_units),
min_stable_power = mean(min_stable_power),
co2_emission = mean(co2_emission),
variable_cost = mean(variable_cost), # Include other relevant columns
start_cost = mean(start_cost),
.groups = 'drop'
)
print(summary)
#There is no reason that commenting these lines should pose a problem...
# Prepend the country and fuel type to the generated name
summary <- summary %>%
mutate(
generator_name = truncateStringVec(combined_names, 88),
# generator_name = paste0(node, "_", gsub(" ", "-", cluster_type), "_", generator_name),
# # combined_names = paste0(
# #   unique(getPrefix(generator_name))[1],  # Extract and keep the prefix only once
# #   paste(
# #     unique(sapply(generator_name, removePrefix)),  # Remove the prefix and combine unique names
# #     collapse = "_"
# #   )
# # )
node = node, # the lines seem silly, but they're actually deeply necessary
# for accessing node and cluster type within the nested df, and aggregate
cluster_type = cluster_type
)
print(summary)
#return(summary)
return(summary %>% select(-node, -cluster_type, -combined_names))  # Exclude node and cluster_type
# Will need to remove node and cluster_type,
# although it was useful back when aggregating and such,
# the fact that it is duplicated will cause an error while unnesting later
}
# Apply clustering and summarization
clustering_test <- thermal_aggregated_tbl %>%
# filter(cluster_type == "Hard Coal") %>%# for testing purposes, will speed things up
group_by(node, cluster_type) %>%
nest() %>%
mutate(
clustered_data = map(data, ~ cluster_and_summarize(.x, k = 5, node, cluster_type))
# the number k can be changed and generalized !
) %>%
# unnest_wider(clustered_data) %>%
unnest(clustered_data) %>%
select(generator_name, node, cluster_type, nominal_capacity, nb_units, min_stable_power, co2_emission, variable_cost, start_cost)
# select(-data)  # Remove `data` after unnesting
# View the result
print(clustering_test, n = 200)
# Un test du 10-clustering (on est g√©n√©reux !) sur le Hard Coal fait passer
# de 1636 lignes √† 971.
# Essayons sur chaque type de thermique maintenant !
# Warning message:
#   There was 1 warning in `mutate()`.
# i In argument: `clustered_data = map(data, ~cluster_and_summarize(.x, k = 10, node, cluster_type))`.
# i In group 201: `node = "AS-IND-EA"` and `cluster_type = "Hard Coal"`.
# Caused by warning:
#   ! did not converge in 10 iterations
# Bon pour l'avoir print, c'est bel et bien un warning et √ßa a bien converg√©
# pour passer de 10 √† 17 du coup
# Un test du 10-clustering sur tous les thermiques a fait passer le tableau
# de 7697 lignes √† 4053.
# saveRDS(object = clustering_test, file = ".\\src\\objects\\thermal_10clustering_tbl.rds")
# clustering_test <- readRDS(".\\src\\objects\\thermal_10clustering_tbl.rds")
# print(clustering_test)
########
# It works !!
# The smart thing to do would be to generalize this method in the script right now.
# The lazy thing to do is to create an r_object and just run a simulation right now.
# Hm..
# Nevermind it doesn't work anymore..
# deja aggregate without clustering √ßa serait pas mal nn ? mdrrr
# faudrait faire un AGGREGATE_THERMAL et AGGREGATE_AND_CLUSTER_THERMAL
# tout comme un AGGREGATE_BATTERIES tout simple √ßa reviendrait juste
# √† changer la r√®gle / la boucle for
##########
# I have a question concerning clustering algorithms.
# Is it possible to generate automatically the amount of clusters,
# depending on the dataset and what would "make sense" intuitively ?
# For example, if I have values 10, 11, 10, 100, 110,
# it's pretty clear that 2 clusters is a good answer.
# But that might change from dataset to dataset.
## R√©ponse : oui mais c'est encore plus long. Il faut run plusieurs fois des k-means
# pour le quantifier. Pas tr√®s int√©ressant comme exercice ici par rapport
# √† juste dire "limiter √† 10 max" par exemple.
#
# Yes, automatically determining the optimal number of clusters in a dataset is a common challenge in clustering tasks. Several methods can help determine a reasonable number of clusters, each with its strengths and limitations. Here are some of the most commonly used methods:
#
#   1. Elbow Method
# The Elbow Method involves running k-means clustering with a range of cluster numbers
# ùëò
# k and plotting the sum of squared distances (within-cluster sum of squares) against
# ùëò
# k. The idea is to find the "elbow" point where adding more clusters yields only a small decrease in the sum of squared distances. The optimal number of clusters is often at this point.
# Apply the clustering and summarization within each country and fuel type
# clustering_test <- thermal_generators_properties_tbl %>%
#   group_by(node, cluster_type) %>%
#   nest()
#
# print(clustering_test)
#
#
# clustering_test <- clustering_test %>%
#   # mutate(
#   #   clustered_data = map2(data, node, cluster_type,
#   #                         ~ cluster_and_summarize(.x, k = 5, node = .y, cluster_type = ..3))
#   # ) %>%
#   mutate(clustered_data = map(data, cluster_and_summarize, k = 2, node, cluster_type)) %>%
#   unnest(clustered_data) %>%
#   select(-data)
# # View the result
# print(clustering_test)
# df <- thermal_generators_properties_tbl
# # Apply the clustering and summarization within each country and fuel type
# clustering_test <- df %>%
#   group_by(node, cluster_type) %>%
#   nest()
#
# print(clustering_test)
#
# clustering_test <- clustering_test %>%
#   mutate(clustered_data = map(data, cluster_and_summarize, k = 2, node, cluster_type)) %>%  # 'data' is the name of the nested tibble column
#   unnest(clustered_data) %>%
#   select(-data)  # Remove the original nested column
# # View the result
# print(clustering_test)
thermiques_tous <- readRDS(".\\src\\objects\\thermal_generators_properties_tbl.rds")
print(thermiques_tous)
asia_nodes <- readRDS(".\\src\\objects\\asia_nodes_lst.rds")
print(asia_nodes)
print(thermiques_tous %>% filter(node %in% asia_nodes))
print(cluster_test)
print(clustering_test)
saveRDS(object = clustering_test, file = ".\\src\\objects\\thermal_5clustering_tbl.rds")
clustering_test <- readRDS(".\\src\\objects\\thermal_5clustering_tbl.rds")
print(clustering_test)
## Script principal regroupant le coeur du processus ##
## Les objets sont typiquement d√©j√† d√©finis dans les fichiers auxilliaires
# Charger les packages
library(antaresRead)
library(antaresEditObject)
# Limite √ßa le EditObject pourrait √™tre limit√© au CreateStudy
# Bon AntaresRead je vois pas comment faire sans mdr
# Penser √† faire comme un requirements.txt genre le truc comme dans logging
# si pas packages alors les installer, et dire dans le README "au pire localement"
# Demander d'ailleurs si on peut virer "antares" du Gitignore
# pour d√©mocratiser AntaresWeb (mais vu que c'est Nicolas qui me l'a fil√©...)
# Importer des fonctions et variables auxilliaires cr√©√©es dans d'autres scripts
antaresFunctions_file = file.path("src", "antaresFunctions.R",
fsep = .Platform$file.sep)
source(antaresFunctions_file)
# Est-ce qu'on regroupe aussi les noms de modules dans les param√®tres ?
# Est-ce qu'on s√©pare param√®tres, faisant un dossier param√®tres ? eh
addNodes_file = file.path("src", "data", "addNodes.R",
fsep = .Platform$file.sep)
source(addNodes_file)
logging_module = file.path("src", "logging.R",
fsep = .Platform$file.sep)
source(logging_module)
setRam(16)
# source("parameters.R")
if (EXPORT_TO_OUTPUT_FOLDER) {
output_dir <- paste0("./output/", generateName("run"))
if (!dir.exists(output_dir)) {
dir.create(output_dir)
}
}
# apparemment le format h5 sert √† compresser tout √ßa ?
# # Convert your study in h5 format
# writeAntaresH5(path = mynewpath)
#
# # Redefine sim path with h5 file
# opts <- setSimulationPath(path = mynewpath)
# prodStack(x = opts)
################################################################################
################################# CREATE STUDY #################################
if (CREATE_STUDY) {
antaresCreateStudy_module = file.path("src", "antaresCreateStudy.R",
fsep = .Platform$file.sep)
source(antaresCreateStudy_module)
}
# Sah quel plaisir for it to run so smoothly now.
# Still gotta implement hydro, however.
# NEXT STEP FOR HYDRO :
# (pendant que j'envoie des sommes de capacit√© √† nicolas chef oui chef)
# impl√©menter les objets Generator avec _Hyd_ en prenant les facteurs de charge mensuels
# en en faisant des TS horaires
# en faisant * max capacit√© * units
# et en mettant tout √ßa dans Run of River
# (not gonna lie, les autres propri√©t√©s, je sais pas ce qu'on en fait)
# MDRR C'EST PAS INDIVIDUEL PAR CONTRE c'est juste on va aggregate tous les RoR ensemble
# enfin j'ai l'impression
# √† redemander avant √† Nicolas
# g√©nial
# et, les objets Battery de type PHS
# bah en vrai y a pas midi √† 14h en terme de nombre de propri√©t√©s
# ce qui est pas clair dans ma t√™te √† la rigueur c'est diff entre
# injection, soutirage, stock, efficacit√©
# (et surtout c'est pas redondant ? genre injection = stock * efficacit√© nn ?)
# AH NON SI OK J'AI je crois
# capacit√© c'est √©norme c'est la maxi taille du r√©servoir genre 34800
# injection c'est oulah √ßa peut pas non plus fournir infini MW dans le r√©seau √† un instant t
# et du coup c'est le max power qui ici est √† 182
# il faut plut√¥t faire d'ailleurs un objet par units parce que y a pas de "unit√©s"
# dans antares batteries
################################################################################
############################### LAUNCH SIMULATION ##############################
if (LAUNCH_SIMULATION) {
# Peut-√™tre ici mettre les logs globaux ce qui permettrait de mettre genre
# starting simulation..
# ou skipped simulation... skipped reading results... done !
antaresLaunchSimulation_module = file.path("src", "antaresLaunchSimulation.R",
fsep = .Platform$file.sep)
source(antaresLaunchSimulation_module)
}
################################################################################
################################## READ RESULTS ################################
if (READ_RESULTS) {
antaresReadResults_module = file.path("src", "antaresReadResults.R",
fsep = .Platform$file.sep)
source(antaresReadResults_module)
}
################################################################################
# Commentaires vari√©s
# Ca existe de stocker des objets R qqpart ?
# Ca peut diminuer le temps de fetch renewables.ninja, surtout quand on a peu de points.
# Mais, c'est peut-√™tre plus risqu√© en un sens, je sais pas.
# Dans input un dossier "Robjects" ou quoi √ßa pourrait le faire.
# D'apr√®s stackoverflow :
#   You can use saveRDS and readRDS functions:
#
#     library(tibble)
#   test <- tibble(a= list(c(1,2), c(3,4)))
#   saveRDS(test, "c:/test.rds")
#   test_2 <- readRDS("c:/test.rds"))
# identical(test, test_2)
# In the readr package there are read_rds and write_rds functions, which even allow compression to be set.
# if (ADD_VOLL) {
#   addVoLL_module = file.path("src", "data", "addVoLL.R")
#   source(addVoLL_module)
#   addVoLLToAntares(nodes, study_path, study_name, log_verbose, console_verbose, fullLog_file, errorsLog_file)
#   message = paste(Sys.time(), "- [MAIN] Done adding VoLL !")
#   log_message(message, fullLog_file, console_verbose)
# }
# La suite : lancer une simulation et la visionner
# Sachant que le visionnage peut √™tre un truc bien √† faire dans un second temps
# Ce qu'il faut faire en fait c'est r√©ussir √† stocker genre des presets
# (dossiers studies tout pr√™ts dans inputs ?)
# et pr√©voir de lancer des simulations, de visionner des r√©sultats dans un second temps
# (des presets de simulation en fait aussi)
# (m√™me pour tester des fonctions Viz de toute fa√ßon ce sera mieux)
## Script principal regroupant le coeur du processus ##
## Les objets sont typiquement d√©j√† d√©finis dans les fichiers auxilliaires
# Charger les packages
library(antaresRead)
library(antaresEditObject)
# Limite √ßa le EditObject pourrait √™tre limit√© au CreateStudy
# Bon AntaresRead je vois pas comment faire sans mdr
# Penser √† faire comme un requirements.txt genre le truc comme dans logging
# si pas packages alors les installer, et dire dans le README "au pire localement"
# Demander d'ailleurs si on peut virer "antares" du Gitignore
# pour d√©mocratiser AntaresWeb (mais vu que c'est Nicolas qui me l'a fil√©...)
# Importer des fonctions et variables auxilliaires cr√©√©es dans d'autres scripts
antaresFunctions_file = file.path("src", "antaresFunctions.R",
fsep = .Platform$file.sep)
source(antaresFunctions_file)
# Est-ce qu'on regroupe aussi les noms de modules dans les param√®tres ?
# Est-ce qu'on s√©pare param√®tres, faisant un dossier param√®tres ? eh
addNodes_file = file.path("src", "data", "addNodes.R",
fsep = .Platform$file.sep)
source(addNodes_file)
logging_module = file.path("src", "logging.R",
fsep = .Platform$file.sep)
source(logging_module)
setRam(16)
# source("parameters.R")
if (EXPORT_TO_OUTPUT_FOLDER) {
output_dir <- paste0("./output/", generateName("run"))
if (!dir.exists(output_dir)) {
dir.create(output_dir)
}
}
# apparemment le format h5 sert √† compresser tout √ßa ?
# # Convert your study in h5 format
# writeAntaresH5(path = mynewpath)
#
# # Redefine sim path with h5 file
# opts <- setSimulationPath(path = mynewpath)
# prodStack(x = opts)
################################################################################
################################# CREATE STUDY #################################
if (CREATE_STUDY) {
antaresCreateStudy_module = file.path("src", "antaresCreateStudy.R",
fsep = .Platform$file.sep)
source(antaresCreateStudy_module)
}
# Sah quel plaisir for it to run so smoothly now.
# Still gotta implement hydro, however.
# NEXT STEP FOR HYDRO :
# (pendant que j'envoie des sommes de capacit√© √† nicolas chef oui chef)
# impl√©menter les objets Generator avec _Hyd_ en prenant les facteurs de charge mensuels
# en en faisant des TS horaires
# en faisant * max capacit√© * units
# et en mettant tout √ßa dans Run of River
# (not gonna lie, les autres propri√©t√©s, je sais pas ce qu'on en fait)
# MDRR C'EST PAS INDIVIDUEL PAR CONTRE c'est juste on va aggregate tous les RoR ensemble
# enfin j'ai l'impression
# √† redemander avant √† Nicolas
# g√©nial
# et, les objets Battery de type PHS
# bah en vrai y a pas midi √† 14h en terme de nombre de propri√©t√©s
# ce qui est pas clair dans ma t√™te √† la rigueur c'est diff entre
# injection, soutirage, stock, efficacit√©
# (et surtout c'est pas redondant ? genre injection = stock * efficacit√© nn ?)
# AH NON SI OK J'AI je crois
# capacit√© c'est √©norme c'est la maxi taille du r√©servoir genre 34800
# injection c'est oulah √ßa peut pas non plus fournir infini MW dans le r√©seau √† un instant t
# et du coup c'est le max power qui ici est √† 182
# il faut plut√¥t faire d'ailleurs un objet par units parce que y a pas de "unit√©s"
# dans antares batteries
################################################################################
############################### LAUNCH SIMULATION ##############################
if (LAUNCH_SIMULATION) {
# Peut-√™tre ici mettre les logs globaux ce qui permettrait de mettre genre
# starting simulation..
# ou skipped simulation... skipped reading results... done !
antaresLaunchSimulation_module = file.path("src", "antaresLaunchSimulation.R",
fsep = .Platform$file.sep)
source(antaresLaunchSimulation_module)
}
################################################################################
################################## READ RESULTS ################################
if (READ_RESULTS) {
antaresReadResults_module = file.path("src", "antaresReadResults.R",
fsep = .Platform$file.sep)
source(antaresReadResults_module)
}
################################################################################
# Commentaires vari√©s
# Ca existe de stocker des objets R qqpart ?
# Ca peut diminuer le temps de fetch renewables.ninja, surtout quand on a peu de points.
# Mais, c'est peut-√™tre plus risqu√© en un sens, je sais pas.
# Dans input un dossier "Robjects" ou quoi √ßa pourrait le faire.
# D'apr√®s stackoverflow :
#   You can use saveRDS and readRDS functions:
#
#     library(tibble)
#   test <- tibble(a= list(c(1,2), c(3,4)))
#   saveRDS(test, "c:/test.rds")
#   test_2 <- readRDS("c:/test.rds"))
# identical(test, test_2)
# In the readr package there are read_rds and write_rds functions, which even allow compression to be set.
# if (ADD_VOLL) {
#   addVoLL_module = file.path("src", "data", "addVoLL.R")
#   source(addVoLL_module)
#   addVoLLToAntares(nodes, study_path, study_name, log_verbose, console_verbose, fullLog_file, errorsLog_file)
#   message = paste(Sys.time(), "- [MAIN] Done adding VoLL !")
#   log_message(message, fullLog_file, console_verbose)
# }
# La suite : lancer une simulation et la visionner
# Sachant que le visionnage peut √™tre un truc bien √† faire dans un second temps
# Ce qu'il faut faire en fait c'est r√©ussir √† stocker genre des presets
# (dossiers studies tout pr√™ts dans inputs ?)
# et pr√©voir de lancer des simulations, de visionner des r√©sultats dans un second temps
# (des presets de simulation en fait aussi)
# (m√™me pour tester des fonctions Viz de toute fa√ßon ce sera mieux)
